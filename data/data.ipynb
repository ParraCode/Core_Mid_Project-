{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduccion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Impotando Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creacion de `df` utilizando datos de `CoreCode` en `data_core/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Preparacion dataset `confirmed_global.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_confirmed_global = \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_confirmed_global.csv&filename=time_series_covid19_confirmed_global.csv\"\n",
    "#df1 = pd.read_csv(url_confirmed_global)\n",
    "df1 = pd.read_csv('data_core/confirmed_global.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El analisis se va a hacer por pais, no por provincia de modo que elimino la columna 'Province/State'. Las columnas de 'Lat' y 'Long' \n",
    "# se van a eliminar ahora para luego mergearlas con el dataframe final, ya que las coordenadas se cerian alteradas en el 'groupby'.\n",
    "\n",
    "df1 = df1.drop(['Province/State'], axis=1)\n",
    "df1 = df1.drop(['Lat'], axis=1)\n",
    "df1 = df1.drop(['Long'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez eliminada dichas columnas agrupamos los datos a nivel de fila por pais Sumando asi todos \n",
    "# los casos por pais que anteriormente estaban subdivididos por 'Province/State'.\n",
    "\n",
    "# Comprobamos que efectivamente, hay nombres de paises que aparecen varias veces\n",
    "print(df1[\"Country/Region\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1[\"Country/Region\"] == \"Austria\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que tras el groupby los casos de agrupado correctamente, ya que la suma de la columna de casos de un dia especifico\n",
    "# es igual a la fila de ese mismo dia para df1 tras esta operacion\n",
    "df1 = df1.groupby(['Country/Region']).sum().reset_index()\n",
    "print(df1.loc[df1[\"Country/Region\"] == \"Austria\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vemos que solo existe un valor por pais. \n",
    "print(df1[\"Country/Region\"].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "Confirmamos que el groupby se ha completado con exito\n",
    "<div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mergeamos las columnas de 'Date-Countrty' por cada pais y anadimos una columna con su valor correspondiente\n",
    "\n",
    "# Agrupo las columnas de fecha en filas utilizando la funcion `melt` y hago un idetificador unico para mergear con el resto\n",
    "# de tablas, que sera el (dia)+(el nombre del pais) para poder mergear correctamente con el resto de tablas por dia y pais\n",
    "df1 = df1.melt(id_vars=[\"Country/Region\"], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Confirmed\")\n",
    "\n",
    "# Creo la columna con el identificador para usarla como indentificador unico para el mergeo\n",
    "df1['Date-Country'] = df1['Date'] + df1['Country/Region']\n",
    "\n",
    "# Hago esta misma columna indice del dataframe\n",
    "df1.set_index('Date-Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Preparacion dataset `deaths_global.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetimos el mismo proceso anterior para el dataset `deaths_global.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_deaths_global = \"https://data.humdata.org/hxlproxy/api/data-preview.csv?url=https%3A%2F%2Fraw.githubusercontent.com%2FCSSEGISandData%2FCOVID-19%2Fmaster%2Fcsse_covid_19_data%2Fcsse_covid_19_time_series%2Ftime_series_covid19_deaths_global.csv&filename=time_series_covid19_deaths_global.csv\"\n",
    "#df2 = pd.read_csv(url_deaths_global)\n",
    "df2 = pd.read_csv('data_core/deaths_global.csv')\n",
    "\n",
    "df2 = df2.drop(['Province/State'], axis=1)\n",
    "df2 = df2.drop(['Lat'], axis=1)\n",
    "df2 = df2.drop(['Long'], axis=1)\n",
    "df2 = df2.groupby(['Country/Region']).sum().reset_index()\n",
    "df2 = df2.melt(id_vars=[\"Country/Region\"], \n",
    "        var_name=\"Date\", \n",
    "        value_name=\"Deaths\")\n",
    "df2['Date-Country'] = df2['Date'] + df2['Country/Region']\n",
    "\n",
    "df2.set_index('Date-Country')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Juntamos todos los dataframe `df1` y `df2` en uno solo `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo un primer dataframe final (df_f1), mergeando df1 y df2 por 'Date-Country'\n",
    "df = pd.merge(df1, df2 , how='left', on='Date-Country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elimino las columnas duplicadas\n",
    "df = df.drop(['Date-Country','Country/Region_y','Date_y'], axis=1)\n",
    "\n",
    "# Reordeno las Columnas\n",
    "df = df.rename(columns={'Country/Region_x':'country', 'Date_x':'date','Confirmed':'totalConfirmed','Deaths':'totalDeaths'})\n",
    "df = df[['country','date','totalConfirmed','totalDeaths']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df.date)\n",
    "df = df.sort_values(['country','date'], ascending=[True, True])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['confirmedDay'] = df['totalConfirmed'].diff().fillna(0).astype(int)\n",
    "df['deathsDay'] = df['totalDeaths'].diff().fillna(0).astype(int)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anado datos geograficos y poblacion a `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Importancion de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv(\"data_extra/concap.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Preaparacion del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df4.rename(columns={'CountryName':'country',\n",
    "                          'CapitalLatitude':'latitude', \n",
    "                          'CapitalLongitude':'longitude', \n",
    "                          'CountryCode':'geoId',\n",
    "                          'ContinentName':'continentExp'})\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df4.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df4: {df4.shape[0]}\\nFilas df4 sin duplicados: {df_DD.shape[0]}\")\n",
    "\n",
    "n_duplicados = df4.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "\n",
    "df4 = df4.dropna()\n",
    "n_null = df4.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control de calidad OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df4 , how='left', on='country')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df4: {df.shape[0]}\\nFilas df4 sin duplicados: {df_DD.shape[0]}\")\n",
    "\n",
    "n_duplicados = df.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "\n",
    "df = df.dropna()\n",
    "n_null = df.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control de calidad ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Filtro `df` por paises de `Europa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_europe = df['continentExp'] == 'Europe'\n",
    "df = df[filter_europe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['continentExp'], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe prepardo para mergearlo y ser enriquecido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Union del dataset: `owid-covid-data.csv` con `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Creacion de `id` en `df` para el mergeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino la columna que me serviran para mergear con otros dataset\n",
    "df['id-merge'] = df['country'] + df['date'].apply(str)\n",
    "df['id-merge'] = df['id-merge'].apply(lambda x: x.split(' ')[0])\n",
    "df['id-merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "n_null = df.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Preaparcion del dataset `owid-covid-data.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1. Importacion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#url_confirmed_global = \"https://covid.ourworldindata.org/data/owid-covid-data.csv\"\n",
    "#df_ex = pd.read_csv(url_confirmed_global)\n",
    "df_ex = pd.read_csv('data_extra/owid-covid-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. Tratamiento de las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elijo las columnas que son relevantes\n",
    "df_ex = df_ex.drop(df_ex.columns.difference(['continent','location', 'date', 'icu_patients','hosp_patients',\n",
    "                                          'total_tests','positive_rate','tests_per_case',\n",
    "                                          'new_vaccinations','people_vaccinated_per_hundred',\n",
    "                                          'people_fully_vaccinated_per_hundred','population']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex = df_ex.rename(columns={'icu_patients':'icuPatients',\n",
    "                          'hosp_patients':'hospPatients', \n",
    "                          'total_tests':'totalTests', \n",
    "                          'positive_rate':'positiveRate',\n",
    "                          'tests_per_case':'testsPerCase',\n",
    "                          'new_vaccinations':'newVaccinations',\n",
    "                          'people_vaccinated_per_hundred':'vaccinatedPerHundred', \n",
    "                          'people_fully_vaccinated_per_hundred':'fullyVaccinatedPerHundred',\n",
    "                          'location':'country'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. Filtro el dataframe por paises `Europeos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter_europe = df_ex['continent'] == 'Europe'\n",
    "#df_ex = df_ex[filter_europe]\n",
    "\n",
    "# Elimino la columna de 'continent'\n",
    "df_ex = df_ex.drop(['continent'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex['date'] = pd.to_datetime(df_ex.date)\n",
    "df_ex = df_ex.sort_values(['country','date'], ascending=[True, True])\n",
    "df_ex = df_ex.reset_index(drop=True)\n",
    "df_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.4. Control de Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df_ex.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df_ex.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df_ex.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "n_null = df_ex.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")\n",
    "print(df_ex.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que hay un grancantidad de registros nulos, que mercen implementar acciones para eliminarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.5. Relleno de Registros Nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Signifcado de las columnas del dataset**\n",
    "- **icu_patients**: Número de pacientes con COVID-19 en unidades de cuidados \n",
    "intensivos (UCI) en un día determinado\n",
    "\n",
    "- **hosp_patients**: Número de pacientes con COVID-19 en el hospital en un día determinado\n",
    "\n",
    "- **total_tests**: Pruebas totales para COVID-19\n",
    "\n",
    "- **positive_rate**: La proporción de pruebas de COVID-19 que son positivas, expresada como un \n",
    "promedio móvil de 7 días (esto es lo contrario de las pruebas por caso)\n",
    "\n",
    "- **tests_per_case**: Pruebas realizadas por cada nuevo caso confirmado de COVID-19, dado como un \n",
    "promedio móvil de 7 días (esto es lo contrario de Positive_rate)\n",
    "\n",
    "- **new_vaccinations**: Nuevas dosis de vacuna COVID-19 administradas \n",
    "(solo calculadas para días consecutivos)\n",
    "\n",
    "- **people_vaccinated_per_hundred**: Número total de personas que recibieron al menos una dosis \n",
    "de vacuna por cada 100 personas en la población total\n",
    "\n",
    "- **people_fully_vaccinated_per_hundred**: Número total de personas que recibieron todas las dosis prescritas por el \n",
    "protocolo de vacunación por cada 100 personas en la población total\n",
    "\n",
    "- **population**: poblacion total del pais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.5.1 Columnas `IcuPatients`,  `hospPatients`, `positive_rate`, `tests_per_case` y `new_vaccinations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser valores No continuos y por dia, es decir que los registros de cada linea es aislado del resto e indivisual, los registros nulos debemos de rellenarlos con `0` puesto que no tenmos mas informacion. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relleno valores nulos con 0\n",
    "df_ex['icuPatients'] = df_ex['icuPatients'].fillna(0)\n",
    "df_ex['hospPatients'] = df_ex['hospPatients'].fillna(0)\n",
    "df_ex['positiveRate'] = df_ex['positiveRate'].fillna(0)\n",
    "df_ex['testsPerCase'] = df_ex['testsPerCase'].fillna(0)\n",
    "df_ex['newVaccinations'] = df_ex['newVaccinations'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio tipo de dato a entero\n",
    "df_ex['icuPatients'] = df_ex['icuPatients'].astype(int)\n",
    "df_ex['hospPatients'] = df_ex['hospPatients'].astype(int)\n",
    "df_ex['newVaccinations'] = df_ex['newVaccinations'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control de calidad de nulos\n",
    "print(df_ex['icuPatients'].isnull().sum())\n",
    "print(df_ex['hospPatients'].isnull().sum())\n",
    "print(df_ex['positiveRate'].isnull().sum())\n",
    "print(df_ex['testsPerCase'].isnull().sum())\n",
    "print(df_ex['newVaccinations'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control de Calidad ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.5.2. Columnas `totalTests`, `vaccinatedPerHundred` y `fullyVaccinatedPerHundred`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser valores acumulativo continuos por dia, los registros nulos debemos de rellenarlos con el registro anterior puesto que no tenemos mas informacion. \n",
    "Los Registros del primer dia en este momento son nulos. Rellenando ese dia con 0 despues podemos rellenarlos con el valor anterior, ya que en caso de que no haya datos, por lo mneos podemos decir que nop habran cambiado y no sera del todo incorrecto ya que son valores que van escalando con el paso del tiempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con esta nueva columnas sabremos cuando cambia de pais a nivel de fila. Ya que al ser el registo de country diferente al anterior, match sera igual 0 \n",
    "df_ex['match'] = df_ex.country == df_ex.country.shift()\n",
    "\n",
    "df_ex['match'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio tipo de dato a match de boll a str\n",
    "df_ex['match'] = df_ex['match'].astype(str)\n",
    "\n",
    "# Replace de bool a 1 y 0\n",
    "df_ex['match'] = df_ex['match'].replace('False', '0')\n",
    "df_ex['match'] = df_ex['match'].replace('True', '1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuando match sea 0 el primer registro de un pais es 0. De esta forma puedo hacer un df.fillna(method='pad') sin que afecte a otros paises ya que el primer registro es 0\n",
    "df_ex.loc[df_ex.match == '0', 'totalTests'] = '0'\n",
    "df_ex.loc[df_ex.match == '0', 'vaccinatedPerHundred'] = '0'\n",
    "df_ex.loc[df_ex.match == '0', 'fullyVaccinatedPerHundred'] = '0'\n",
    "df_ex.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex['totalTests'] = df_ex['totalTests'].fillna(method='pad')\n",
    "df_ex['vaccinatedPerHundred'] = df_ex['vaccinatedPerHundred'].fillna(method='pad')\n",
    "df_ex['fullyVaccinatedPerHundred'] = df_ex['fullyVaccinatedPerHundred'].fillna(method='pad')\n",
    "df_ex.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df_ex.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df_ex.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df_ex.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "n_null = df_ex.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")\n",
    "print(df_ex.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex['totalTests'] = df_ex['totalTests'].astype(int)\n",
    "df_ex['vaccinatedPerHundred'] = df_ex['vaccinatedPerHundred'].astype(float)\n",
    "df_ex['fullyVaccinatedPerHundred'] = df_ex['fullyVaccinatedPerHundred'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1.5.3. Columna `population`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex[df_ex['population'].isnull() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex[df_ex['population'].isnull() == True]['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que los nulos correspondes a paises que no interesan para el analisis, ya que no se encuentran en `df`. Por lo tanto los eliminaremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ex =  df_ex.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df_ex.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df_ex.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df_ex.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "n_null = df_ex.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")\n",
    "print(df_ex.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control de calidad ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambio population a int\n",
    "df_ex['population'] = df_ex['population'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.6. Creando `id` para mergear con `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defino la columna que me serviran para mergear con otros dataset\n",
    "df_ex['id-merge'] = df_ex['country'] + df_ex['date'].apply(str)\n",
    "df_ex['id-merge'] = df_ex['id-merge'].apply(lambda x: x.split(' ')[0])\n",
    "df_ex['id-merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimino columnas duplicados con df\n",
    "df_ex = df_ex.drop(['country', 'date', 'match'], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Control de calidad ok. \n",
    "Dataframe listo para mergear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Enriquecimiento de `df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos inner porque no me interesan registros que esten en un df y en otro no. Son dos datasets que se van actualizando diariamente pero puedo hacer diferencias entre paises en los ultimos dias. pero no resulta sigficativo para dicha tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mergeo con el dataset de test\n",
    "df = pd.merge(df, df_ex, how='inner', on='id-merge')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DD = df.drop_duplicates()\n",
    "\n",
    "print(f\"Filas df: {df.shape[0]}\\nFilas df sin duplicados: {df_DD.shape[0]}\")\n",
    "n_duplicados = df.shape[0] - df_DD.shape[0]\n",
    "print(f\"Hay {n_duplicados} filas duplicadas\")\n",
    "n_null = df.isnull().sum().sum()\n",
    "print(f\"Hay {n_null} registros nulos en total\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df['country'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_min = df['date'].min()\n",
    "date_max = df['date'].max()\n",
    "dias = date_max - date_min\n",
    "print(date_min,date_max, dias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Exportacion `df` to `.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d0493199427ef061cb3b6912ede36f63d4e203655e7b69a0bcc0db7ae4af873e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
